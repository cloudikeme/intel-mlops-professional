{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0541c12e-12f7-4d23-bc14-0590672c6979",
   "metadata": {},
   "source": [
    "# Leveraging Intel Optimizations with Hugging Face for Enhanced Model Performance\n",
    "\n",
    "<img src=\"https://www.developer-tech.com/wp-content/uploads/sites/3/2023/08/intel-pytorch-foundation-ai-development-artificial-intelligence-coding-programming-machine-learning.jpg\" alt=\"Alt Text\" style=\"width: 400px;\"/>\n",
    "\n",
    "This notebook serves as an introduction to utilizing IPEX for fine-tuning a pre-trained model, specifically focusing on the `distilbert-base-uncased` model for multi-class emotion classification in text. In part two of this lab you will learn how to contribute the open source Hugging Face model hub.  \n",
    "\n",
    "## Why This is Important\n",
    "\n",
    "Understanding how to leverage Intel optimizations is crucial for developers looking to maximize computational efficiency and performance. By integrating IPEX with Hugging Face's API, we can significantly enhance training speeds, especially when utilizing mixed precision training with FP32 and BF16. This notebook will demonstrate these capabilities practically, offering insights into:\n",
    "\n",
    "- How to enable IPEX within Hugging Face's `TrainingArguments` and training functions.\n",
    "- Comparing training speeds and efficiencies between IPEX-enabled and standard training processes.\n",
    "- Performing inference to assess the model's accuracy in classifying emotions.\n",
    "\n",
    "## Acquiring the Learnings\n",
    "\n",
    "Through step-by-step instructions, hands-on examples, and comparative analyses, this workshop will equip you with the skills to effectively integrate Intel's optimizations into your NLP projects using Hugging Face. Let's dive into the world where cutting-edge language processing meets optimized computational performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501a047-ca9b-41d9-a1ca-ec3e1613c573",
   "metadata": {},
   "source": [
    "#### Environment Setup and Dependencies Installation\n",
    "\n",
    "This cell prepares our working environment. It sources Intel oneAPI for optimal performance on Intel hardware (optional based on your setup) and installs specific versions of essential libraries: `transformers`, `torch`, and `intel_extension_for_pytorch`. These installations ensure we have the necessary tools to leverage Intel's optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/intel/oneapi/setvars.sh: No such file or directory\n",
      "Collecting transformers==4.35.2\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers==4.35.2) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers==4.35.2) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers==4.35.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers==4.35.2) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.35.2)\n",
      "  Downloading regex-2024.7.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers==4.35.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
      "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.35.2)\n",
      "  Downloading safetensors-0.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.35.2)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers==4.35.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers==4.35.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers==4.35.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers==4.35.2) (2024.7.4)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Downloading regex-2024.7.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (790 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m790.9/790.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.4 tokenizers-0.15.2 tqdm-4.66.5 transformers-4.35.2\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement intel_extension_for_pytorch==2.1.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for intel_extension_for_pytorch==2.1.0\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting datasets==2.16.1\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from datasets==2.16.1) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from datasets==2.16.1) (2.0.1)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.16.1)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.1)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets==2.16.1) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from datasets==2.16.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets==2.16.1) (4.66.5)\n",
      "Collecting xxhash (from datasets==2.16.1)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.16.1)\n",
      "  Downloading aiohttp-3.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets==2.16.1) (0.24.6)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from datasets==2.16.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading multidict-6.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2024.7.4)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading aiohttp-3.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
      "Downloading multidict-6.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Downloading yarl-1.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 datasets-2.16.1 dill-0.3.7 frozenlist-1.4.1 fsspec-2023.10.0 multidict-6.0.5 multiprocess-0.70.15 pyarrow-17.0.0 pyarrow-hotfix-0.6 xxhash-3.5.0 yarl-1.9.4\n",
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate==0.26.0) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate==0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from accelerate==0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/codespace/.local/lib/python3.12/site-packages (from accelerate==0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from accelerate==0.26.0) (2.3.1+cpu)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/python/3.12.1/lib/python3.12/site-packages (from accelerate==0.26.0) (0.24.6)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from accelerate==0.26.0) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub->accelerate==0.26.0) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Downloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.26.0\n"
     ]
    }
   ],
   "source": [
    "!source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "!pip install transformers==4.35.2\n",
    "!pip install torch==2.1.0\n",
    "!pip install intel_extension_for_pytorch==2.1.0\n",
    "!pip install datasets==2.16.1\n",
    "!pip install accelerate==0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d9300-aa72-4cb2-bfd9-ee5c2ca391af",
   "metadata": {},
   "source": [
    "#### Loading Libraries and Packages\n",
    "\n",
    "In this cell, we import the core libraries that will be used throughout the notebook. This setup is crucial as it prepares our Python environment with all the necessary tools for our tasks.\n",
    "\n",
    "- `from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments`: We import key components from the Hugging Face Transformers library. `AutoModelForSequenceClassification` and `AutoTokenizer` are used for loading the model and tokenizer, respectively. `Trainer` and `TrainingArguments` are essential for setting up and running our model training.\n",
    "- `from datasets import load_dataset`: This import from the `datasets` library allows us to easily load and preprocess datasets available in Hugging Face's datasets hub.\n",
    "- `import numpy as np`: Numpy is a fundamental package for scientific computing in Python. It provides support for arrays, mathematical operations, and various utility functions.\n",
    "- `from sklearn.metrics import accuracy_score`: We import the `accuracy_score` function from Scikit-Learn to calculate the accuracy of our model predictions during evaluation. This metric will help us quantify the performance of our fine-tuned model.\n",
    "\n",
    "Overall, this cell lays the foundation for our machine learning tasks by equipping us with the necessary libraries and modules.t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4acc2-5808-449f-a55b-bd3484ad3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea22e9f-4e4e-4672-bdba-b264f5893d41",
   "metadata": {},
   "source": [
    "#### Dataset Loading\n",
    "\n",
    "Here, we load the `emotion` dataset from Hugging Face's datasets library. This dataset will be used for training and evaluating our DistilBERT model, providing a practical context for emotion classification in text.k..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89638882-e384-46a7-93c4-de9b60d33e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc4f80-7806-4bc8-b251-dd3d8e1d5189",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Initialization\n",
    "\n",
    "In this cell, we initialize the `distilbert-base-uncased` model and its corresponding tokenizer for sequence classification. This setup is the first step in preparing our model for fine-tuning on the emotion classification task..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882dcc84-5752-489f-9e0d-adcd1b8201a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad31f1-2669-4018-9629-ff6ba02af7d3",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "Data preprocessing is essential for model training. We define and apply a preprocessing function that tokenizes our text data, making it compatible with the DistilBERT model's input requirements.\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b861c-54c8-4a43-95dc-94975ac1ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf58cd-55a0-4bec-88e5-930beecf5e78",
   "metadata": {},
   "source": [
    "#### Training with IPEX\n",
    "\n",
    "This cell is where the integration of Intel Extension for PyTorch (IPEX) comes into play. We define training arguments, including enabling BF16 and IPEX, and set up our Hugging Face trainer. The model is then trained on the emotion dataset, utilizing the enhanced capabilities provided by IPEX..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0798d9-7d90-41c0-ac7b-4d1b0370873d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    bf16=True, \n",
    "    use_ipex=True,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1046c73-336d-4edb-9210-71a50571583e",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "Post-training, we evaluate the model's performance on the validation dataset. This evaluation will give us insights into the effectiveness of our training and the accuracy of the model in emotion classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7678f5-385d-44fc-857b-c501712ae8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7c017-dcdf-4d79-8441-0eb3eb9ae50f",
   "metadata": {},
   "source": [
    "#### Inference and Testing\n",
    "\n",
    "Finally, we test the fine-tuned model's inference capabilities on new sentences. This step involves preprocessing the test sentences, performing predictions, and mapping these predictions to human-readable labels. It allows us to visually inspect the model's ability to classify emotions in various text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ea670-26d0-439c-8b1f-e2156ba8f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sentences\n",
    "test_sentences = [\n",
    "    \"I am feeling incredibly happy and joyful today!\",\n",
    "    \"I am so sad and down.\",\n",
    "    \"I have mixed feelings about this.\",\n",
    "    \"This is absolutely terrifying!\",\n",
    "]\n",
    "\n",
    "# Preprocess the test sentences\n",
    "encoded_input = tokenizer(test_sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Predict using the fine-tuned model\n",
    "with torch.no_grad():\n",
    "    predictions = model(**encoded_input)\n",
    "\n",
    "# Convert predictions to human-readable labels\n",
    "predicted_labels = np.argmax(predictions.logits.numpy(), axis=1)\n",
    "\n",
    "# Mapping for the 'emotion' dataset labels\n",
    "label_map = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
    "\n",
    "# Print predictions\n",
    "for sentence, label_id in zip(test_sentences, predicted_labels):\n",
    "    print(f\"Sentence: '{sentence}' - Emotion Prediction: {label_map[label_id]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46160e25-50d6-48b6-8353-851320c95794",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Throughout this workshop, we have explored the integration of Intel optimizations with Hugging Face's powerful Transformers library. By fine-tuning the DistilBERT model with the support of Intel Extension for PyTorch, we observed enhanced training speeds and efficient utilization of computational resources, especially notable in mixed precision training scenarios.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The exercise showcased not only the technical prowess of combining Hugging Face with Intel optimizations but also highlighted the practical benefits such as reduced training times and resource efficiency. This understanding is pivotal for developers working on NLP tasks, seeking to optimize model performance on Intel hardware. As AI and NLP continue to evolve, harnessing these optimizations will be key in developing more efficient and powerful AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
